<html>
<head>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
    <style>
        h1 {
            text-align: center;
        }

        .container {
            margin: 0 auto;
            padding: 60px 20%;
        }

        figure {
            text-align: center;
        }

        img {
            display: inline-block;
        }

        body {
            font-family: 'Inter', sans-serif;
        }
    </style>
</head>
<body>
<div class="container">
    <h1>CS184/284A Spring 2025 Homework 3 Write-Up</h1>
    <div style="text-align: center;">Names: Ethan Zhang and Ray Zhang</div>

    <br>

    Link to webpage: <a href="https://cal-cs184-student.github.io/hw-webpages-sendhelp2/hw3/">cal-cs184-student.github.io/hw-webpages-sendhelp2/hw3/</a>

    <br>

    Link to GitHub repository: <a href="https://github.com/cal-cs184-student/hw-webpages-sendhelp2">github.com/cal-cs184-student/hw-webpages-sendhelp2</a>

    <!--
    We've already added one heading per part, to make your write-up as navigable when grading. Please fit your write-up within these sections!
    -->

    <h2>Overview</h2>

    For this homework, we built upon a basic rendering engine by implementing ray generation / intersection, direct
    illumination, and global illumination. Along the way, we also optimized a lot of the path/ray tracing computations
    by creating a Bounding Volume Hierarchy, adding russian roulette sampling, and adaptive sampling. In the end, we are
    able to create images from a virtual scene where a camera tries to capture light bouncing from various objects in a
    scene according to mathematical estimates. In addition, we brought in some of our knowledge from previous homeworks
    to find object intersections and general spatial awareness in terms of the world / camera coordinates.
    <br> <br>
    All in all, we learned a lot about the reasoning behind optimizations and found a lot of fun doing the renders to
    see the color and scene come to life from the previous flat renders. (I also found it really cool how optimizations
    can speed up renders by several magnitudes).
    <div style="break-after: page;"></div>
    <h2>Part 1: Ray Generation and Scene Intersection</h2>

    <h3>A walk through of the ray generation and primitive intersection parts of the rendering pipeline</h3>
    To start building our path tracer, we needed to first generate the rays that we will cast out towards the scene
    from the prospective of the camera. In particular, we want to somewhat model the pin-hole idea by having an origin
    where rays are casted out of. Then, we can determine which point on the camera sensor we want the rays to hit (in
    other words, we created the ray in camera space first). Since we have objects described in the scene space, we
    further used the <code>c2w</code> matrix to transform the ray into world space. <br><br>
    Now that we have the ray, we need to cast it out multiple times into the scene (from slightly different points on
    the target pixel for sampling purposes). For each ray, we then need to check if it intersect any objects in the
    scene and, if so, we estimate the illuminance that the object is currently receiving (and thus will reflect back
    to the camera via the bsdf estimator).

    <h3>Our triangle intersection algorithm</h3>
    In order to reuse our point-in-triangle test from homework 1, we needed to first convert the problem to something
    we were familiar with. Specifically, we needed to find where the point would be on the triangle, and more
    importantly, project the 3D triangle down to the 2D space. To do this, we decided to use the ray intersection with
    plane equation (in which the normal was the cross product of two triangle vertices) to determine the time, t, at
    which the ray would intersect the triangle. Now that we have the point in 3D space, we proceed to project all 4
    points (+ triangle vertices) to an arbitrary plane. With all the points on the same plane, we reused the 3 line
    tests from hw1 to test whether the projected plane-intersection points fall with-in the triangle. <br><br>
    Lastly, we used barycentric coordinates from the same homework to somewhat average out the vertex normals to return
    the final intersection normal.
    <br>
    <h3>Images with normal shading for a few small .dae files:</h3>
    <div style="display: flex; flex-direction: column; align-items: center;">
        <table style="width: 100%; text-align: center; border-collapse: collapse;">
            <tr>
                <td style="text-align: center;">
                    <img src="images/1-banana.png" width="400px"/>
                    <figcaption>banana.dae</figcaption>
                    <br>
                </td>
                <td style="text-align: center;">
                    <img src="images/1-CBcoil.png" width="400px"/>
                    <figcaption>CBcoil.dae</figcaption>
                    <br>
                </td>
            </tr>
            <tr>
                <td style="text-align: center;">
                    <img src="images/1-teapot.png" width="400px"/>
                    <figcaption>teapot.dae</figcaption>
                </td>
                <td style="text-align: center;">
                    <img src="images/1-CBspheres_lambertian.png" width="400px"/>
                    <figcaption>CBspheres_lambertian.dae</figcaption>
                </td>
            </tr>
        </table>
    </div>
    <br>
    <br>
    <br>
    <div style="break-after: page;"></div>
    <h2>Part 2: Bounding Volume Hierarchy</h2>

    <h3>BVH construction algorithm walk through & heuristics</h3>

    Since the idea of BVH is to construct somewhat of a 3D binary tree, we thought it made the most sense to split each
    large bounding volume into two halves with roughly equal probability of a light ray hitting it. To estimate this, we
    picked the splitting heuristic of the average of primitives' centroids. Specifically, we also measured the extent
    of the overall bounding volume for the considered primitives first to find the axis which had the most spread. After
    a small adjustment to give less weight to the Z-axis (since we reasoned that light rays will likely traverse both
    halves), we split the primitives by their centroids on the chosen axis against the average.<br><br>
    Once we figured out the splitting scheme, the construction was quite simple: find the average centroid, pick axis
    to split, split by the primitives' centroids into two child nodes, and recurse. Then, to quickly find ray
    intersections, it was easy to start from the root node, then for the given node, we see if the ray hits and only
    if it hits, we consider the primitives if its a leaf node, otherwise the left and right children nodes, and recurse.
    <br>
    <h3>Large .dae files that could only be rendered in reasonable time with BVH acceleration:</h3>
    <div style="display: flex; flex-direction: column; align-items: center;">
        <table style="width: 100%; text-align: center; border-collapse: collapse;">
            <tr>
                <td style="text-align: center;">
                    <img src="images/2-walle.png" width="400px"/>
                    <figcaption>walle.dae (240326 primitives)</figcaption>
                </td>
                <td style="text-align: center;">
                    <img src="images/2-building.png" width="400px"/>
                    <figcaption>building.dae (39506 primitives)</figcaption>
                </td>
                <td style="text-align: center;">
                    <img src="images/2-bunny.png" width="400px"/>
                    <figcaption>bunny.dae (33696 primitives)</figcaption>
                </td>
            </tr>
        </table>
    </div>
    <br>
    <br>
    <br>

    <h3>Rendering times on a few scenes with moderately complex geometries with and without BVH acceleration</h3>

    <details>
        <summary>PathTracer Output</summary>
        <pre>
    Part 2: BEFORE
    [PathTracer] Input scene file: ../dae/meshedit/beetle.dae
    [PathTracer] Rendering... 100%! (65.9512s)
    BVH traced 480000 rays.
    Building BVH from 7558 primitives
    Average speed 0.0073 million rays per second
    [PathTracer] Averaged 7558.000000 intersection tests per ray.

    [PathTracer] Input scene file: ../dae/sky/CBcoil.dae
    Building BVH from 7884 primitives... Done! (0.0000 sec)
    54.2154s
    [PathTracer] BVH traced 480000 rays.
    [PathTracer] Average speed 0.0089 million rays per second.
    [PathTracer] Averaged 7884.000000 intersection tests per ray.

    [PathTracer] Input scene file: ../dae/sky/bunny.dae
    [PathTracer] Building BVH from 33696 primitives... Done! (0.0007 sec)
    [PathTracer] Rendering... 100%! (454.1831s)
    [PathTracer] BVH traced 480000 rays.
    [PathTracer] Average speed 0.0011 million rays per second.
    [PathTracer] Averaged 33696.000000 intersection tests per ray.

    Part 2: AFTER
    [PathTracer] Input scene file: ../dae/meshedit/beetle.dae
    [PathTracer] Building BVH from 7558 primitives... Done! (0.0050 sec)
    [PathTracer] Rendering... 100%! (0.0753s)
    [PathTracer] BVH traced 480000 rays.
    [PathTracer] Average speed 6.3714 million rays per second.
    [PathTracer] Averaged 1.474331 intersection tests per ray.

    [PathTracer] Input scene file: ../dae/sky/CBcoil.dae
    [PathTracer] Building BVH from 7884 primitives... Done! (0.0067 sec)
    [PathTracer] Rendering... 100%! (0.0883s)
    [PathTracer] BVH traced 480000 rays.
    [PathTracer] Average speed 5.4352 million rays per second.
    [PathTracer] Averaged 1.949583 intersection tests per ray.

    [PathTracer] Input scene file: ../dae/sky/bunny.dae
    [PathTracer] Building BVH from 33696 primitives... Done! (0.0292 sec)
    [PathTracer] Rendering... 100%! (0.1113s)
    [PathTracer] BVH traced 480000 rays.
    [PathTracer] Average speed 4.3119 million rays per second.
    [PathTracer] Averaged 2.595469 intersection tests per ray.
    </pre>
    </details>
    <br>
    BVH speeds up the rendering of complex scenes by several magnitudes, and noticeably improves performance even at
    smaller scales. For testing, we tried to render three different scenes: beetle, CBcoil, and bunny. For
    <code>beetle.dae</code>, a moderately small scene with 7,558 primitives, the improvement was already apparent with
    BVH acceleration taking only 0.0753 seconds to render in contrast to 65.9512s without it. This improvement is even
    more significant in a large scene like <code>bunny.dae</code> with 33,696 primitives reducing the time from 7
    minutes to just 0.1113 seconds. In order to understand why this happens, we take a look at the average
    intersection tests per ray. For <code>bunny.dae</code>, without BVH acceleration, we see 33,696 intersection tests,
    one for each primitive in the scene whilst BVH reduces this to just 2.59 intersection tests. The previous tests
    are clearly inefficient since the light ray could only reasonably hit a certain area in the scene so the BVH reduces
    a lot of inefficient tests. In addition, in scenes with less occlusions (from camera's prospective) like
    <code>beetle.dae</code>, we get even more performance than roughly same sized <code>CBcoil</code> since it is more
    likely for a ray to terminate before hitting another bounding box.
    <br>
    <div style="break-after: page;"></div>
    <h2>Part 3: Direct Illumination</h2>

    <h3>Direct lighting implementations walk through</h3>
    In order to render images with realistic shading, we implement two different lighting estimators: hemispherical
    sampling and importance sampling. With both sampling, we still cast out rays for each pixel of the final image.
    However, the difference lies in the handling of light intersecting with objects in the scene. For uniform
    hemispherical sampling, we sample <code>l</code> random directions from essentially a half-dome shape around the
    normal of the surface at the intersection point. Then, we do a "shadow" ray trace from this direction to get
    the average reflectance at that intersection point. Even though our results already looked pretty realistic, the
    major limitation is that it was very noisy with many bounce samples never reaching a light source. <br><br>
    To reduce the likelihood of bounces never reaching light, we implemented importance sampling of lights where, in
    bounces, we strictly sample from the solid angle covering each light. To do this, we iterate through each light
    present in the scene and get samples of angles that would hit the light (along with the pdf which tells us how
    likely it was to sample this angle). In this way, the samples converged a lot quickly and the image became a lot
    less noisy.
    <br>
    <h3>Some images rendered with both implementations of the direct lighting function</h3>
    For some scenes, the objects use a different bsdf than implemented which causes it to become dark in some renders
    but the effects of its shadow still showcases the difference between importance and hemispherical sampling.
    <br>
    <br>
    <div style="display: flex; flex-direction: column; align-items: center;">
        <table style="width: 100%; text-align: center; border-collapse: collapse;">
            <tr>
                <td style="text-align: center;">
                    <img src="images/3-CBbunny_64_32.png" width="400px"/>
                    <figcaption>CBbunny.dae (importance sampling)</figcaption>
                    <br>
                </td>
                <td style="text-align: center;">
                    <img src="images/3-CBbunny_H_64_32.png" width="400px"/>
                    <figcaption>CBbunny.dae (uniform hemisphere sampling)</figcaption>
                    <br>
                </td>
            </tr>
            <tr>
                <td style="text-align: center;">
                    <img src="images/3-CBcoil_64_32.png" width="400px"/>
                    <figcaption>CBcoil.dae (importance sampling)</figcaption>
                    <br>
                </td>
                <td style="text-align: center;">
                    <img src="images/3-CBcoil_H_64_32.png" width="400px"/>
                    <figcaption>CBcoil.dae (uniform hemisphere sampling)</figcaption>
                    <br>
                </td>
            </tr>
            <tr>
                <td style="text-align: center;">
                    <img src="images/3-CBdragon_64_32.png" width="400px"/>
                    <figcaption>CBdragon.dae (importance sampling)</figcaption>
                </td>
                <td style="text-align: center;">
                    <img src="images/3-CBdragon_H_64_32.png" width="400px"/>
                    <figcaption>CBdragon.dae (uniform hemisphere sampling)</figcaption>
                </td>
            </tr>
        </table>
    </div>
    <br>
    <br>
    <br>
    <h3>Area light with different amounts of sampled light rays</h3>
    Focusing on the CB spheres (with lambertian bsdf) scene since it had at least one area light. We rendered it with 1,
    4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling. In terms of
    noise levels in the soft shadow, it seemed to decrease quickly with the amount of lights ray that we used to sample.
    I believe this is due to the occlusion of the spheres being slightly included/not depending on the sampled location
    on the pixel for raytracing:
    <br>
    <br>
    <div style="display: flex; flex-direction: column; align-items: center;">
        <table style="width: 100%; text-align: center; border-collapse: collapse;">
            <tr>
                <td style="text-align: center;">
                    <img src="images/3-CBspheres_lambertian-l1s1_64_32.png" width="400px"/>
                    <figcaption>CBspheres_lambertian.dae (1 light ray)</figcaption>
                    <br>
                </td>
                <td style="text-align: center;">
                    <img src="images/3-CBspheres_lambertian-l4s1_64_32.png" width="400px"/>
                    <figcaption>CBspheres_lambertian.dae (4 light rays)</figcaption>
                    <br>
                </td>
            </tr>
            <tr>
                <td style="text-align: center;">
                    <img src="images/3-CBspheres_lambertian-l16s1_64_32.png" width="400px"/>
                    <figcaption>CBspheres_lambertian.dae (16 light rays)</figcaption>
                </td>
                <td style="text-align: center;">
                    <img src="images/3-CBspheres_lambertian-l64s1_64_32.png" width="400px"/>
                    <figcaption>CBspheres_lambertian.dae (64 light rays)</figcaption>
                </td>
            </tr>
        </table>
    </div>
    <br>
    <br>
    <br>
    <h3>Uniform hemisphere sampling vs. importance lighting sampling</h3>
    In the above results, it is clear that in the same amount of light samples, importance light sampling resulted
    in a much less noisy image--indicating that it has found an estimate of the lighting values at certain parts on
    the scene faster than uniform hemispherical sampling. However, this is only achieved by biasing and restricting
    the sample to just the lighting sources which is possible because we can give a good description to the distribution
    representing angles that could hit the light. In contrast, uniform hemisphere sampling with enough sampling would
    accurately reconstruct an unbiased result. As seen though, even with 64+ light samples, the hemispherical sampling
    was still very noisy and it is clear that some samples have hit the light source while others haven't even though
    they are both in areas not occluded by the sphere. I can also imagine if a scene has a lot of lights and
    partitions, it might not be practical to go through every light and some sort of spatial hashing might be necessary.
    Lastly, uniform hemisphere sampling may also never capture the light emission from a point source since the solid
    angle for such light would practically be zero.
    <div style="break-after: page;"></div>
    <h2>Part 4: Global Illumination</h2>

    <h3>Indirect lighting function walk through</h3>

    With just one bounce lighting, our shadows were quite harsh and it didn't make sense for the ceiling to be pitch
    dark. In order to implement 1+ bounces for ray tracing, it was first necessary to implement a way to sample an
    input direction and output another direction representing the additional bounces between a light source and the
    camera. Then, we could begin to accumulate the all the light along a path up to a certain depth. For most testing,
    we chose a depth of 5 and how it works is we get the light emission like previous part at the last depth. Then, we
    kind of trace backwards to find the light reflectance given each surfaces' bsdf and divided by the pdf corresponding
    to the probability of that sample. Lastly, we add up all the contributions from each depth to get the final
    radiance estimate. In order to estimate how much radiance would be provided given bounce lighting of infinite depth,
    we implemented russian roulette global illumination where at each depth, we used <code>p_rr_terminate = 0.35</code>
    to determine whether to stop early. If it is an early stop, we could multiply by the continuity probability to get
    a rough estimate of what it would look like if it continued further.
    <br>
    <h3>Some images rendered with global (direct and indirect) illumination, using 1024 samples per pixel</h3>
    <div style="display: flex; flex-direction: column; align-items: center;">
        <table style="width: 100%; text-align: center; border-collapse: collapse;">
            <tr>
                <td style="text-align: center;">
                    <img src="images/4-bench.png" width="400px"/>
                    <figcaption>bench.dae</figcaption>
                    <br/>
                </td>
                <td style="text-align: center;">
                    <img src="images/4-blob.png" width="400px"/>
                    <figcaption>blob.dae</figcaption>
                    <br>
                </td>
            </tr>
            <tr>
                <td style="text-align: center;">
                    <img src="images/4-dragon.png" width="400px"/>
                    <figcaption>dragon.dae</figcaption>
                </td>
                <td style="text-align: center;">
                    <img src="images/4-spheres.png" width="400px"/>
                    <figcaption>CBSpheres.dae</figcaption>
                </td>
            </tr>
        </table>
    </div>
    <br>
    <br>
    <br>
    <h3><code>Bunny.dae</code> with only direct illumination / only indirect illumination, using
    1024 samples per pixel</h3>
    <br>
    <br>
    <div style="display: flex; flex-direction: column; align-items: center;">
        <table style="width: 100%; text-align: center; border-collapse: collapse;">
            <tr>
                <td style="text-align: center;">
                    <img src="images/4-1-bunny_direct.png" width="400px"/>
                    <figcaption>bunny.dae (direct illumination)</figcaption>
                </td>
                <td style="text-align: center;">
                    <img src="images/4-1-bunny_indirect.png" width="400px"/>
                    <figcaption>bunny.dae (indirect illumination)</figcaption>
                </td>
            </tr>
        </table>
    </div>
    <br>
    <br>
    <br>
    <h3><code>CBbunny.dae</code> with mth bounce of light, having max_ray_depth set to 0, 1, 2, 3, 4, and 5 (the -m flag),
        with and without isAccumBounces=false.</h3>
    In the 2nd and 3rd bounce of light, it seems to add in a lot of the color information from the two side walls,
    filling in the floors and the side of the bunnies with color. This contributes significantly to the realism and
    quality of the image since it makes sense that the bunny would kind of absorb some of the color from the walls.
    In addition, the shadow under the bunny becomes a lot less intense from these additional bouncing of light
    and the corners also became less dark with more bounced lights. It was also really cool to see the ceiling be filled
    up with light bounced from the scene since before it was just completely dark.
    <br>
    <br>
    <div style="display: flex; flex-direction: column; align-items: center;">
        <table style="width: 100%; text-align: center; border-collapse: collapse;">
            <tr>
                <td colspan="6">
                    <code>isAccumBounces=false</code>
                </td>
            </tr>
            <tr>
                <td style="text-align: center;">
                    <img src="images/4-2-bunny0.png" width="150px"/>
                    <figcaption>0th bounce</figcaption>
                </td>
                <td style="text-align: center;">
                    <img src="images/4-2-bunny1.png" width="150px"/>
                    <figcaption>1st bounce</figcaption>
                </td>
                <td style="text-align: center;">
                    <img src="images/4-2-bunny2.png" width="150px"/>
                    <figcaption>2nd bounce</figcaption>
                </td>
                <td style="text-align: center;">
                    <img src="images/4-2-bunny3.png" width="150px"/>
                    <figcaption>3rd bounce</figcaption>
                </td>
                <td style="text-align: center;">
                    <img src="images/4-2-bunny4.png" width="150px"/>
                    <figcaption>4th bounce</figcaption>
                </td>
                <td style="text-align: center;">
                    <img src="images/4-2-bunny5.png" width="150px"/>
                    <figcaption>5th bounce</figcaption>
                </td>
            </tr>
            <tr>
                <td colspan="6">
                    <br>
                    <code>isAccumBounces=true</code>
                </td>
            </tr>
            <tr>
                <td style="text-align: center;">
                    <img src="images/4-2-bunny0-acc-no-rr.png" width="150px"/>
                    <figcaption>0th bounce</figcaption>
                </td>
                <td style="text-align: center;">
                    <img src="images/4-2-bunny1-acc-no-rr.png" width="150px"/>
                    <figcaption>..1st bounce</figcaption>
                </td>
                <td style="text-align: center;">
                    <img src="images/4-2-bunny2-acc-no-rr.png" width="150px"/>
                    <figcaption>..2nd bounce</figcaption>
                </td>
                <td style="text-align: center;">
                    <img src="images/4-2-bunny3-acc-no-rr.png" width="150px"/>
                    <figcaption>..3rd bounce</figcaption>
                </td>
                <td style="text-align: center;">
                    <img src="images/4-2-bunny4-acc-no-rr.png" width="150px"/>
                    <figcaption>..4th bounce</figcaption>
                </td>
                <td style="text-align: center;">
                    <img src="images/4-2-bunny5-acc-no-rr.png" width="150px"/>
                    <figcaption>..5th bounce</figcaption>
                </td>
            </tr>
        </table>
    </div>
    <br>
    <br>
    <br>

    <h3>Russian Roulette rendering for CBbunny.dae with max_ray_depth set to 0, 1, 2, 3, 4, and 100(the -m
    flag), using 1024 samples per pixel</h3>
    <br>
    <br>
    <div style="display: flex; flex-direction: column; align-items: center;">
        <table style="width: 100%; text-align: center; border-collapse: collapse;">
            <tr>
                <td style="text-align: center;">
                    <img src="images/4-3-bunny0-rr.png" width="150px"/>
                    <figcaption>depth: 0</figcaption>
                </td>
                <td style="text-align: center;">
                    <img src="images/4-3-bunny1-rr.png" width="150px"/>
                    <figcaption>depth: 1</figcaption>
                </td>
                <td style="text-align: center;">
                    <img src="images/4-3-bunny2-rr.png" width="150px"/>
                    <figcaption>depth: 2</figcaption>
                </td>
                <td style="text-align: center;">
                    <img src="images/4-3-bunny3-rr.png" width="150px"/>
                    <figcaption>depth: 3</figcaption>
                </td>
                <td style="text-align: center;">
                    <img src="images/4-3-bunny4-rr.png" width="150px"/>
                    <figcaption>depth: 4</figcaption>
                </td>
                <td style="text-align: center;">
                    <img src="images/4-3-bunny100-rr.png" width="150px"/>
                    <figcaption>depth: 100</figcaption>
                </td>
            </tr>
        </table>
    </div>
    <br>
    <br>
    <br>

    <h3>Rendered <code>wall-e.dae</code> views with various sample-per-pixel rates, including 1, 2, 4, 8, 16,
    64, and 1024, using 4 light rays</h3>

    In the following renders, we see that, with more sample-per-pixel, we get a lot less noise on the surface of the
    object. Lighting-wise, it seems like the softness of the shadows don't change much.

    <br>
    <br>
    <div style="display: flex; flex-direction: column; align-items: center;">
        <table style="width: 100%; text-align: center; border-collapse: collapse;">
            <tr>
                <td style="text-align: center;">
                    <img src="images/4-6-wall-e_s1.png" width="400px"/>
                    <figcaption>1 sample-per-pixel</figcaption>
                    <br>
                </td>
                <td style="text-align: center;">
                    <img src="images/4-6-wall-e_s2.png" width="400px"/>
                    <figcaption>2 sample-per-pixel</figcaption>
                    <br>
                </td>
            </tr>
            <tr>
                <td style="text-align: center;">
                    <img src="images/4-6-wall-e_s4.png" width="400px"/>
                    <figcaption>4 sample-per-pixel</figcaption>
                    <br>
                </td>
                <td style="text-align: center;">
                    <img src="images/4-6-wall-e_s8.png" width="400px"/>
                    <figcaption>8 sample-per-pixel</figcaption>
                    <br>
                </td>
            </tr>
            <tr>
                <td style="text-align: center;">
                    <img src="images/4-6-wall-e_s16.png" width="400px"/>
                    <figcaption>16 sample-per-pixel</figcaption>
                    <br>
                </td>
                <td style="text-align: center;">
                    <img src="images/4-6-wall-e_s64.png" width="400px"/>
                    <figcaption>64 sample-per-pixel</figcaption>
                    <br>
                </td>
            </tr>
            <tr>
                <td style="text-align: center;">
                    <img src="images/4-6-wall-e_s1024.png" width="400px"/>
                    <figcaption>1024 sample-per-pixel</figcaption>
                </td>
            </tr>
        </table>
    </div>
    <br>
    <br>
    <br>
    <div style="break-after: page;"></div>
    <h2>Part 5: Adaptive Sampling</h2>
    <h3>Adaptive sampling walk through</h3>
    While rendering the scene, it became clear that some regions of the scene were not that complicated. Flat areas
    like the wall often only bounced towards the light and there were not many other elements that would interact with
    it. In this sense, most of the samples from bounces would return similar values and became redundant. To estimate
    this redundancy, we looked at the variance of the distribution of the sample illuminances every so often. With the
    variance of the distribution and the number of samples, we estimate
    <code>I = 1.96 &sdot; &sigma; / sqrt(n)</code> as the unit of
    convergence for that pixel. Every <code>samplesPerBatch</code>, we would then check if this <code>I</code> would
    get above a certain value based on the mean * maxTolerance, representing the 95% confidence interval.
    If it has, then we can stop sampling for that pixel.
    <br>
    <h3><code>Bench.dae</code> and <code>dragon.dae</code> rendered with adaptive sampling</h3>
    Below, we show <code>bench.dae</code> and <code>dragon.dae</code> rendered at 2048 samples-per-pixels, 1 sample
    per light, and m = 6. For both renders, it is clear that in regions close to complex curves and creases, the
    adaptive sampling rate is much higher. This makes intuitive sense since there are a lot more close-by surface in
    which light can be scattered, depending on the sampling location, it could very well include or not include
    contributions from these surfaces. Thus, it would take longer to converge. In contrast, the flat surfaces further
    from edges had "green" sampling rate which makes sense since it is not very affected by surroundings and each sample
    probably is around the same.
    <br>
    <br>
    <div style="display: flex; flex-direction: column; align-items: center;">
        <table style="width: 100%; text-align: center; border-collapse: collapse;">
            <tr>
                <td style="text-align: center;">
                    <img src="images/5-bench.png" width="400px"/>
                    <figcaption>bench.dae (noise-free rendered result)</figcaption>
                    <br>
                </td>
                <td style="text-align: center;">
                    <img src="images/5-bench_rate.png" width="400px"/>
                    <figcaption>bench.dae (sample rate image)</figcaption>
                    <br>
                </td>
            </tr>
            <tr>
                <td style="text-align: center;">
                    <img src="images/5-dragon.png" width="400px"/>
                    <figcaption>dragon.dae (noise-free rendered result)</figcaption>
                </td>
                <td style="text-align: center;">
                    <img src="images/5-dragon_rate.png" width="400px"/>
                    <figcaption>dragon.dae (sample rate image)</figcaption>
                </td>
            </tr>
        </table>
    </div>
    <br>
    <br>
    <br>


    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
</div>
</body>
</html>
